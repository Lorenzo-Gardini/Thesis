\documentclass{article}

% Pacchetti utili
\usepackage[utf8]{inputenc} % Per la codifica dei caratteri
\usepackage[T1]{fontenc} % Per i font
\usepackage{graphicx} % Per le immagini
\usepackage{amsmath, amssymb} % Per la matematica
\usepackage{hyperref} % Per i link
\usepackage{natbib} % Per le citazioni

\begin{document}

\section{Introduzione}

L'algoritmo \textbf{Bayesian Personalized Ranking (BPR)} è stato sviluppato per affrontare il problema del \textit{personalized ranking} a partire da \textit{implicit feedback}, come click, visualizzazioni o acquisti. A differenza degli approcci tradizionali che cercano di prevedere le valutazioni esplicite $r_{ui}$, BPR mira a modellare l'\textit{ordine di preferenza} degli \textit{item} per ciascuno \textit{user} $u$.

\section{Motivazione}

Nei sistemi basati su \textit{feedback impliciti}, la mancanza di interazione tra un utente e un item non implica necessariamente una preferenza negativa. Per affrontare tale ambiguità, BPR adotta le seguenti ipotesi:

\begin{itemize}
    \item Se $(u, i) \in R$, cioè l'utente $u$ ha interagito con l'item $i$, allora $u$ preferisce $i$ a tutti gli item $j$ con cui non ha interagito.
    \item A partire da questo principio, si costruisce un insieme di triple:
    \[
    D_S := \{(u, i, j) \mid i \in I_u \wedge j \in I \setminus I_u\}
    \]
    dove ogni tripla rappresenta la preferenza dell'utente $u$ per l'item $i$ rispetto all'item $j$.
\end{itemize}

\section{Criterio di Ottimizzazione (BPR-Opt)}

L'obiettivo di BPR è massimizzare la probabilità a posteriori dei parametri del modello $\Theta$, dati i dati osservati:
\[
\text{BPR-Opt} = \sum_{(u, i, j) \in D_S} \ln \sigma(\hat{x}_{uij}) - \lambda_\Theta ||\Theta||^2
\]
dove:
\begin{itemize}
    \item $\hat{x}_{uij} = \hat{r}_{ui} - \hat{r}_{uj}$ è la differenza tra le valutazioni stimate per $i$ e $j$;
    \item $\sigma(x) = \frac{1}{1 + e^{-x}}$ è la funzione sigmoide;
    \item $\lambda_\Theta$ è il parametro di regolarizzazione.
\end{itemize}

Questo criterio è strettamente legato all'ottimizzazione dell'AUC (Area Under the ROC Curve), che misura la qualità del ranking.

\section{Algoritmo LearnBPR}

L'ottimizzazione di BPR-Opt viene effettuata tramite \textit{stochastic gradient descent} con campionamento bootstrap:

% \begin{algorithm}[H]
% \caption{LearnBPR}
% \begin{algorithmic}[1]
% \Procedure{LearnBPR}{$D_S, \Theta$}
%     \State Inizializza $\Theta$
%     \Repeat
%         \State Campiona $(u, i, j)$ da $D_S$
%         \State $\Theta \gets \Theta + \alpha \cdot \left( \frac{e^{-\hat{x}_{uij}}}{1 + e^{-\hat{x}_{uij}}} \cdot \frac{\partial \hat{x}_{uij}}{\partial \Theta} - \lambda_\Theta \cdot \Theta \right)$
%     \Until{convergenza}
%     \State \Return $\Theta$
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}

Questo approccio consente una rapida convergenza e un buon bilanciamento tra classi positive e negative.

\section{Applicazioni di BPR}

BPR può essere applicato a diverse famiglie di modelli. Di seguito due esempi noti:

\subsection{Matrix Factorization (BPR-MF)}

Ogni \textit{user} $u$ e \textit{item} $i$ sono rappresentati da vettori latenti $\mathbf{w}_u$ e $\mathbf{h}_i$. La valutazione stimata è:
\[
\hat{r}_{ui} = \langle \mathbf{w}_u, \mathbf{h}_i \rangle = \sum_{f=1}^k w_{uf} \cdot h_{if}
\]
e quindi:
\[
\hat{x}_{uij} = \hat{r}_{ui} - \hat{r}_{uj} = \langle \mathbf{w}_u, \mathbf{h}_i - \mathbf{h}_j \rangle
\]

\subsection{Adaptive k-Nearest-Neighbor (BPR-kNN)}

La stima della preferenza è basata sulla somma delle similarità tra l’item $i$ e gli item già valutati:
\[
\hat{r}_{ui} = \sum_{l \in I_u} c_{il}
\]
\[
\hat{x}_{uij} = \sum_{l \in I_u} (c_{il} - c_{jl})
\]

\section{Vantaggi del BPR}

\begin{itemize}
    \item Ottimizza direttamente l'obiettivo di ranking anziché i rating.
    \item Si adatta bene a scenari con feedback impliciti.
    \item È flessibile e applicabile a vari modelli di raccomandazione.
    \item È empiricamente superiore a metodi classici come WR-MF e SVD.
\end{itemize}

\section{Esempio}

Supponiamo che l’utente $u_1$ abbia interagito con $i_2$ ma non con $i_1$ e $i_4$. Le triple generate saranno:
\[
(u_1, i_2, i_1), \quad (u_1, i_2, i_4)
\]
che esprimono la preferenza implicita di $u_1$ per $i_2$ rispetto agli altri.

\section{Conclusione}

L’approccio BPR introduce un framework bayesiano per l’ottimizzazione del ranking personalizzato, fornendo migliori prestazioni grazie all’adattamento del criterio di apprendimento all’obiettivo finale. Il suo algoritmo LearnBPR è efficiente e facilmente adattabile a diversi modelli.

######################################################

\section*{Bayesian Personalized Ranking - Formulazione Matematica}

\subsection*{1. Modello di Matrix Factorization}

Ogni utente \( u \in U \) e ogni item \( i \in I \) sono rappresentati da un vettore latente in \( \mathbb{R}^k \):

\[
\mathbf{w}_u \in \mathbb{R}^k \quad \text{(utente)} \qquad
\mathbf{h}_i \in \mathbb{R}^k \quad \text{(item)}
\]

Eventualmente, si aggiunge un termine di bias \( b_i \in \mathbb{R} \), trattando il vettore dell'item come:

\[
\mathbf{h}_i = [h_{i1}, h_{i2}, \dots, h_{ik}, b_i] \in \mathbb{R}^{k+1}
\]

\subsection*{2. Costruzione del training set}

Dato un insieme di interazioni osservate \( S \subseteq U \times I \), si costruisce il set di triple:

\[
D_S = \{(u, i, j) \mid (u, i) \in S,\, (u, j) \notin S \}
\]

\subsection*{3. Scoring Function}

Per ogni tripla \( (u, i, j) \), il modello calcola:

\[
\hat{x}_{ui} = \langle \mathbf{w}_u, \mathbf{h}_i \rangle = \sum_{f=1}^{k} w_{uf} \cdot h_{if} + b_i
\]

\[
\hat{x}_{uj} = \langle \mathbf{w}_u, \mathbf{h}_j \rangle = \sum_{f=1}^{k} w_{uf} \cdot h_{jf} + b_j
\]

\[
\hat{x}_{uij} = \hat{x}_{ui} - \hat{x}_{uj} = \langle \mathbf{w}_u, \mathbf{h}_i - \mathbf{h}_j \rangle + (b_i - b_j)
\]

\subsection*{4. Funzione obiettivo (BPR-Opt)}

La funzione obiettivo da massimizzare è:

\[
\text{BPR-Opt} = \sum_{(u,i,j) \in D_S} \log \sigma(\hat{x}_{uij}) 
- \lambda \left( \|\mathbf{w}_u\|^2 + \|\mathbf{h}_i\|^2 + \|\mathbf{h}_j\|^2 + b_i^2 + b_j^2 \right)
\]

dove la funzione sigmoide è definita come:

\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]

\subsection*{5. Aggiornamenti tramite SGD}

Definiamo:

\[
z = \sigma(-\hat{x}_{uij}) = \frac{1}{1 + e^{\hat{x}_{uij}}}
\]

Gli aggiornamenti dei parametri sono:

\begin{align*}
\mathbf{w}_u &\leftarrow \mathbf{w}_u + \alpha \left( z \cdot (\mathbf{h}_i - \mathbf{h}_j) - \lambda \cdot \mathbf{w}_u \right) \\
\mathbf{h}_i &\leftarrow \mathbf{h}_i + \alpha \left( z \cdot \mathbf{w}_u - \lambda \cdot \mathbf{h}_i \right) \\
\mathbf{h}_j &\leftarrow \mathbf{h}_j + \alpha \left( -z \cdot \mathbf{w}_u - \lambda \cdot \mathbf{h}_j \right) \\
b_i &\leftarrow b_i + \alpha (z - \lambda b_i) \\
b_j &\leftarrow b_j + \alpha (-z - \lambda b_j)
\end{align*}

dove:
\begin{itemize}
    \item \( \alpha \) è il learning rate,
    \item \( \lambda \) è il coefficiente di regolarizzazione.
\end{itemize}

\subsection*{6. Previsione finale}

Una volta addestrato il modello, lo score finale per ogni utente-item è:

\[
\hat{x}_{ui} = \langle \mathbf{w}_u, \mathbf{h}_i \rangle + b_i
\]

Gli item vengono ordinati in base a \( \hat{x}_{ui} \) decrescente per costruire il ranking personalizzato.



\bibliographystyle{plain}
\bibliography{../bibliography}

\end{document}
