\chapter{Valutazione}

Tutto il codice della libreria è stato soggetto a \textit{unit test}, che sono stati eseguiti con il framework \textit{Pytest}. Non sono stati implementati \textit{integration test}, poiché la libreria è progettata per essere utilizzata in modo modulare e le interazioni tra i componenti sono state verificate singolarmente. In aggiunta ai test è stata calcolata la percentuale di \textit{coverage} raggiunta utilizzando il tool \textit{coverage}. In totale la \textit{coverage} raggiunta è del 100\%. Il modulo di \texttt{visualization} è stato testato manualmente, poiché non è possibile automatizzare completamente la verifica dei grafici generati. Tuttavia, sono stati effettuati controlli visivi per garantire che i grafici fossero corretti e rappresentativi dei dati.

\textit{Dataset utilizzati per i test}:

\begin{itemize}
    \item \textit{LovieLens-1M}: esempio classico di dataset per sistemi di raccomandazione, con 1 milione di valutazioni di film da parte degli utenti. Contiene anche features sui film e sugli utenti, come generi, età e sesso
    \item \textit{Retailrocket Dataset}: presente su \textit{Kaggle}, contiene dati di interazioni tra utenti e \textit{item} in un negozio online, con informazioni su visualizzazioni, aggiunte al carrello e acquisti. È utile per testare modelli di raccomandazione basati su interazioni implicite
    \item \textit{dataset interno}: contiene dati di interazioni tra utenti e polizze assicurative. Le features degli utenti includono età, sesso e nazionalità, mentre le polizze hanno informazioni su tipo, durata e costo. È utile per testare modelli di raccomandazione in ambito assicurativo
\end{itemize}

\section{Test preprocessing}

Per il testing dei \textit{transformers} sono stati utilizzati un misto di dataset reali e sintetici. I dataset sintetici sono stati creati per coprire casi d'uso specifici, come la presenza di valori mancanti o la necessità di utilizzare le varie tipologie di \textit{binning}. I dataset reali sono stati utilizzati per verificare che i \textit{transformers} funzionassero correttamente con dati del mondo reale. Ogni \textit{transformer} è stato testato rispetto alle operazioni equivalenti implementate in \textit{Pandas}, \textit{NumPy} e \textit{Scikit-learn}. I risultati sono stati confrontati per garantire che la libreria producesse gli stessi output. Per i \textit{transformers} che utilizzano possibili modalità come parametri di input (e.g. \texttt{FillNa} con il campo \texttt{method}) sono state usate singole liste o l'oggetto \texttt{ParameterGrid} di \textit{Scikit-learn} per generare tutte le combinazioni possibili di parametri.

\section{Test modelli}

Per i modelli l'obbiettivo dei test è stato in primis di verificare che i modelli fossero compatibili con la libreria \textit{Scikit-learn}. Questa verifica è stata possibile grazie alla funzione \texttt{check\_estimator} di \textit{Scikit-learn}, che ha permesso di testare i modelli rispetto a un insieme di casi d'uso standardizzati. Successivamente, si è proceduto a testare i modelli rispetto al formato dei dati in ingresso e in uscita, per garantire che i modelli potessero essere utilizzati con il formato standard. Infine si è verificato che ciascun modello riproducesse le stesse predizioni dei modelli delle librerie originali a parità di configurazione. Tutti i test sono stati eseguiti utilizzando i dataset di test descritti in precedenza. Per il modello di \textit{LightFM} sono state testate tutte le combinazioni che in includevano o escludevano le features degli utenti e dei \textit{item}.

Per i modelli che implementano \texttt{similar\_items} e \texttt{similar\_users}:

\begin{itemize}
    \item per i modelli di \textit{Implicit} si è fatto un confronto con i metodi della libreria originale, verificando che i risultati fossero equivalenti
    \item per il modello \textit{LightFM} e i modelli di \textit{Surprise} si è verificato che i risultati fossero equivalenti a quelli ottenuti eseguendo il calcolo della similarità \texttt{cosine\_similarity} di \textit{Scikit-learn} o, nel caso specifico del modello \textit{KNNBaseline}, ai risultati ottenuti dalle funzioni di similarità \\ \texttt{surprise.similarities.pearson} \\ o  \texttt{surprise.similarities.pearson\_baseline} e \\ \texttt{surprise.similarities.msd} di \textit{Surprise}
\end{itemize}

Per testare le varie combinazioni di parametri si è utilizzato l'oggetto \\ \texttt{ParameterGrid} di \textit{Scikit-learn}, che permette di generare tutte le combinazioni possibili di parametri per i modelli. Importante sottolineare che alcuni modelli, soprattutto quelli più complessi come \texttt{SVD++}, richiedono un tempo di addestramento significativo, che può variare da pochi secondi a diversi minuti a seconda della configurazione e del dataset utilizzato. Inoltre bisogna considerare che i modelli delle librerie non godono di ottimizzazioni su \textit{GPU} e la libreria \textit{Surprise} nemmeno di ottimizzazioni \textit{Cython} il che la rende lenta nelle fasi di addestramento e predizione. 

\section{Test evaluation}

Per il modulo di \textit{evaluation} sono stati implementati confrontando il valore della metrica rispetto al valore calcolato in modo grezzo, senza l'utilizzo della libreria. In questo modo si è verificato che le metriche calcolate dalla libreria fossero corrette e coerenti con i risultati attesi. Sono state testate anche le metriche per feedback implicito nella loro versione \textit{Factory} con il modello \textit{LightFM}. Le metriche per feedback esplicito in versione \textit{Factory} sono state testate utilizzando come parametro \texttt{predict\_fn} quella base dei modelli di \textit{Surprise} perché non esiste un modello per dati espliciti che potrebbe necessitare di modifiche custom alla metrica. Importante sottolineare che il tempo di calcolo delle metriche dipende sia dalla velocità di predizione del modello sia dalla dimensione del dataset di test. Inoltre, per le metriche implicite la predizione per ciascun utente viene calcolata su tutto l'insieme di \textit{item}, il che può richiedere un tempo significativo, soprattutto per modelli complessi e dataset di grandi dimensioni.

\section{Test altri moduli}
Per tutte le funzioni presenti nei moduli di \texttt{utils} e \texttt{model\_selection} sono state implementate dalle due alle tre funzioni di test sufficienti per coprire i vari casi d'uso.

\section{Usabilità}
La libreria, un volta completata, è stata testata da un gruppo di utenti interni all'azienda \textit{Data Reply} per valutarne l'usabilità e l'efficacia, soprattutto rispetto alle librerie già esistenti ed utilizzate. I feedback raccolti sono stati utilizzati per apportare miglioramenti e ottimizzazioni alla libreria. Si consideri la differenza di linee di codice tra un utilizzo completo di \textit{LightFM} e l'utilizzo della libreria non includendo il preprocessing dei dati:

\begin{lstlisting}[caption=esempio di utilizzo completo della libreria \textit{LightFM}]
RANDOM_STATE = 1234
K = 10
EPOCHS = 100

# train and predict recommendations using original LightFM model
lightfm_model = lightfm.LightFM(random_state=RANDOM_STATE)
# fit Dataset based on features and data
lightfm_dataset = Dataset()
item_features_data = unique(list([(row['item_id'], f"{col}:{value[col]}") 
                                  for _, value in policies_item.iterrows() 
                                  for col in ['duration', 'cost']]))
user_features_data = unique(list([(row['user_id'], f"{col}:{row[col]}")
                                  for _, row in policies_users.iterrows() 
                                  for col in ['age', 'sex', 'nationality']]))

lightfm_dataset.fit(users=unique(policies_users['user_id']),
                    items=unique(policies_users['item_id']),
                    item_features=item_features_data,
                    user_features=user_features_data)

# create interactions matrix and weights matrix instance for data
interactions, weights = lightfm_dataset.build_interactions(trainset.values)

# train the model with features
lightfm_model.fit(interactions,
                  sample_weight=weights,
                  item_features=item_features,
                  user_features=user_features,
                  epochs=EPOCHS)

# get mapping
user_id_mapping, _, item_id_mapping, _ = lightfm_dataset.mapping()

# creates all ids for prediction
predictions_user_ids = np.array([user_id_mapping[user] 
                                 for user in testset['user_id']])
unchanged_user_ids = [user for user, _ in id_pairs]
predictions_item_ids = np.array([item_id_mapping[item] 
                                 for item in testset['item_id']])
unchanged_item_ids = [item for _, item in id_pairs]

# prediction using LightFM model
lightfm_scores = lightfm_model.predict(predictions_user_ids,
                                       predictions_item_ids,
                                       item_features=item_features,
                                       user_features=user_features)
# zip together unchanged ids and predictions scores
total_prediction = [{'user_id': user_id, 'item_id': item_id, 'interaction_weight': score}
                    for user_id, item_id, score in zip(unchanged_user_ids, unchanged_item_ids,lightfm_scores)]
sort_function = lambda x: (x['interaction_weight'], x['item_id'])
map_function =


def select_predictions(grouped_predictions):
    # sort predictions by ranking
    sorted_predictions = sorted(grouped_predictions, 
                                key=lambda x: (x['interaction_weight'], x['item_id']), 
                                reverse=True)
    # select first k
    k_predictions = sorted_predictions[:K]
    # extract only ids
    mapped_predictions = map(lambda x: x['item_id'], k_predictions)
    return list(mapped_predictions)

# compute groups
groups = {key_id: group for key_id, group in 
          groupby(predictions, lambda x: x['user_id']).items()}

# group by key, and extract for each id top k predictions
rankings = {key_id: select_predictions(grouped_predictions)
            for key_id, grouped_predictions in groups.items()}
\end{lstlisting}

\begin{lstlisting}[caption=esempio di utilizzo del modello \texttt{LightFM} libreria]
RANDOM_STATE = 1234
K = 10
EPOCHS = 100

# create model
new_lightfm_model = LightFM(random_state=RANDOM_STATE)
# train model with
new_lightfm_model.fit(trainset, 
                      item_features=item_features,
                      user_features=user_features, 
                      epochs=EPOCHS)
# create predictions
new_total_prediction = new_lightfm_model.predict(testset, 
                                                 user_features=user_features,
                                                 item_features=item_features)
# create rankings from predictions
new_rankings = create_rankings_from_implicit(dataset=testset,
                                             scores=new_total_prediction, 
                                             k=K, 
                                             is_user_ranking=True)
\end{lstlisting}

Come si può notare, l'utilizzo della libreria consente di ridurre significativamente il numero di linee di codice necessarie per addestrare e utilizzare un modello di \textit{LightFM}, rendendo il codice più leggibile e mantenibile.

\section{Documentazione}
Per garantire la facilità d'uso della libreria, è stata creata una documentazione completa utilizzando \textit{Sphinx} utilizzando il tema \texttt{sphinx\_rtd\_theme}. La documentazione include:

\begin{itemize}
    \item una guida all'installazione e all'utilizzo della libreria
    \item una descrizione dettagliata di ogni modulo e delle sue funzioni
    \item esempi di utilizzo per ciascun modulo
\end{itemize}

\section{CI/CD}

Per garantire la qualità del codice e l'affidabilità della libreria, è stato implementato un sistema di \textit{Continuous Integration/Continuous Deployment} (\textit{CI/CD}) utilizzando la \textit{GitHub Actions} \texttt{astral-sh/setup-uv@v5} per la gestione del tool \textit{uv}. Questo sistema esegue automaticamente i test ogni volta che viene effettuata una modifica con versione del codice, notificando con una mail in caso di fallimento. Si è deciso di utilizzare l'esecuzione dei test solamente per modifiche significative in quanto il tempo di esecuzione dei test è elevato. Inoltre, ha il compito di generare la documentazione della libreria e di pubblicarla su \textit{GitHub pages} e su \textit{Confluence} utilizzando un'integrazione ad hoc.

\section{Limiti della soluzione proposta}

La libreria è stata progettata per essere modulare e flessibile, ma presenta alcuni limiti:

\begin{itemize}
    \item non supporta modelli basati su \textit{deep learning} come \textit{Neural Collaborative Filtering}
    \item tutto il codice è scritto in \textit{Python} nativo o in \textit{Cython}, quindi non godendo di ottimizzazioni su \textit{GPU} può limitare le prestazioni su dataset di grandi dimensioni. Questo potrebbe rendere l'addestramento, la predizione e i test molto lenti, soprattutto per modelli complessi
    \item gli algoritmi proposti, soprattutto quelli di \textit{Surprise}, hanno difficoltà a gestire dataset di dimensioni grandi. Inoltre, utilizzano solamente le informazioni di interazione tra utenti e \textit{item}, senza considerare altre informazioni sull'interazione o le features degli utenti e degli \textit{item} (tranne \textit{LightFM}) che potrebbero migliorare le prestazioni del modello
    \item non offre nessuna funzionalità per calcolare la similarità tra \textit{item} tramite immagini o testi, che sono spesso utilizzate nei sistemi di raccomandazione moderni    
\end{itemize}



